{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a99a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51809453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c9e2b",
   "metadata": {},
   "source": [
    "## Bagging K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30908acc",
   "metadata": {},
   "source": [
    "## Fetch the Wisconsin Breast Cancer dataset with the respective scikit-learn methods and split into training and test set (test ratio of 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d71c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = sklearn.datasets.load_breast_cancer()\n",
    "\n",
    "df_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df_cancer['label'] = cancer.target\n",
    "\n",
    "cancer_X = df_cancer.drop('label', axis=1)\n",
    "cancer_y = df_cancer['label']\n",
    "\n",
    "cancer_X_train, cancer_X_test, cancer_y_train, cancer_y_test = sklearn.model_selection.train_test_split(cancer_X, cancer_y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d8bb0",
   "metadata": {},
   "source": [
    "## Train a k-Means clustering algorithm (ùëò=2) on the training data and inspect its performance on the test set w.r.t. the Adjusted Rand Index (ARI). What does this measure express?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57896c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI is 0.3551488631223801\n"
     ]
    }
   ],
   "source": [
    "kmeans = sklearn.cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(cancer_X_train, cancer_y_train)\n",
    "\n",
    "labels_pred = kmeans.predict(cancer_X_test)\n",
    "\n",
    "ari = sklearn.metrics.cluster.adjusted_rand_score(cancer_y_test, labels_pred)\n",
    "\n",
    "print(\"ARI is\", ari)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14418f12",
   "metadata": {},
   "source": [
    "Rand index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "The adjusted Rand index is ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701c421",
   "metadata": {},
   "source": [
    "## Train a BaggingClassifier with k-Means (ùëò=2) as the base estimator. The classifier should use all features and 30 % of the training examples for each created classifier. Also, it should be allowed to use the same example for different classifier instances. There should be 20 estimators used during bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf1d587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(base_estimator=KMeans(n_clusters=2), max_samples=0.3,\n",
       "                  n_estimators=20, random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(base_estimator=KMeans(n_clusters=2), max_samples=0.3,\n",
       "                  n_estimators=20, random_state=5)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=2)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=2)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(base_estimator=KMeans(n_clusters=2), max_samples=0.3,\n",
       "                  n_estimators=20, random_state=5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = sklearn.ensemble.BaggingClassifier(base_estimator = sklearn.cluster.KMeans(n_clusters=2), max_samples = 0.3, n_estimators = 20, bootstrap=True, random_state = 5)\n",
    "bc.fit(cancer_X_train, cancer_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a561d69",
   "metadata": {},
   "source": [
    "## Evaluate the ARI performance of the bagging classifier. How does it perform? In which scenarios is bagging probably outperforming a single estimator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1e6a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI with bagging is 0.33219165511341014\n"
     ]
    }
   ],
   "source": [
    "pred = bc.predict(cancer_X_test)\n",
    "ari_bc = sklearn.metrics.cluster.adjusted_rand_score(cancer_y_test, pred)\n",
    "print(\"ARI with bagging is\", ari_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3199c37",
   "metadata": {},
   "source": [
    "It does not outperform simple kmeans here, but in general bagging is suitable for high variance low bias models, so to avoid overfitting. It is also useful when we do not have a large sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ee34c",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9544747e",
   "metadata": {},
   "source": [
    "## Generate some synthetic regression data with the respective scikit-learn method. You want to have 1000 samples with 20 features (10 informative ones). Set a standard deviation for the gaussian noise of 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0756dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = sklearn.datasets.make_regression(n_samples=1000, n_features=20, n_informative=10, noise=0.2, random_state=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(features, target, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c6f5f",
   "metadata": {},
   "source": [
    "## Build a pipeline that scales the data and afterwards applies a Random Forest regressor. Set the parameters of the Random Forest to 50 estimators and a maximum number of ten leaf nodes. Train this pipeline and measure its performance on the test set. Use a suitable performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e005863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Random Forest Regressor  8457.827532905352\n"
     ]
    }
   ],
   "source": [
    "sc = sklearn.preprocessing.RobustScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestRegressor(n_estimators=50, max_leaf_nodes=10)\n",
    "rf.fit(X_train_std, y_train)\n",
    "\n",
    "Y_pred = rf.predict(X_test_std)\n",
    "\n",
    "print('MSE for Random Forest Regressor ', sklearn.metrics.mean_squared_error(Y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a9a8b",
   "metadata": {},
   "source": [
    "## Check the importance values of the features estimated by the Random Forest. Are they compliant with the generated data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ccf353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.00000\n",
      "Feature: 1, Score: 0.24205\n",
      "Feature: 2, Score: 0.00000\n",
      "Feature: 3, Score: 0.00000\n",
      "Feature: 4, Score: 0.00000\n",
      "Feature: 5, Score: 0.00296\n",
      "Feature: 6, Score: 0.00000\n",
      "Feature: 7, Score: 0.00860\n",
      "Feature: 8, Score: 0.00000\n",
      "Feature: 9, Score: 0.24133\n",
      "Feature: 10, Score: 0.00000\n",
      "Feature: 11, Score: 0.00000\n",
      "Feature: 12, Score: 0.00000\n",
      "Feature: 13, Score: 0.39075\n",
      "Feature: 14, Score: 0.00000\n",
      "Feature: 15, Score: 0.00000\n",
      "Feature: 16, Score: 0.00000\n",
      "Feature: 17, Score: 0.00000\n",
      "Feature: 18, Score: 0.11432\n",
      "Feature: 19, Score: 0.00000\n"
     ]
    }
   ],
   "source": [
    "importance = rf.feature_importances_\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f7296",
   "metadata": {},
   "source": [
    "The results suggest 5 of the 20 features as being important to prediction, which contradicts our generated data with 10 informative features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058e3e4",
   "metadata": {},
   "source": [
    "## AdaBoost with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38812098",
   "metadata": {},
   "source": [
    "## Fetch the Wisconsin Breast Cancer dataset with the respective scikit-learn methods and split into training and test set (test ratio of 0.2). Scale the data to [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23517b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = sklearn.datasets.load_breast_cancer()\n",
    "\n",
    "df_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df_cancer['label'] = cancer.target\n",
    "\n",
    "cancer_X = df_cancer.drop('label', axis=1)\n",
    "cancer_y = df_cancer['label']\n",
    "\n",
    "cancer_X_train, cancer_X_test, cancer_y_train, cancer_y_test = sklearn.model_selection.train_test_split(cancer_X, cancer_y, test_size=0.2, random_state=1)\n",
    "\n",
    "sc = sklearn.preprocessing.MinMaxScaler()\n",
    "sc.fit(cancer_X_train)\n",
    "cancer_X_train_std = sc.transform(cancer_X_train)\n",
    "cancer_X_test_std = sc.transform(cancer_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e2772",
   "metadata": {},
   "source": [
    "## Train a Logistic Regression that classifies the training data. Use default parameters. What is the performance of the trained model w.r.t. accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "612e8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "logisticRegr = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "logisticRegr.fit(cancer_X_train_std, cancer_y_train)\n",
    "\n",
    "pred1 = logisticRegr.predict(cancer_X_test_std)\n",
    "\n",
    "print(\"Accuracy for Logistic Regression:\", sklearn.metrics.accuracy_score(cancer_y_test, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035129f",
   "metadata": {},
   "source": [
    "## Train an AdaBoostClassifier with Logistic Regression (same configuration as before) as the base estimator. There should be 20 estimators used during boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbfb1f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for AdaBoostClassifier: 0.9035087719298246\n"
     ]
    }
   ],
   "source": [
    "AdaBoost = sklearn.ensemble.AdaBoostClassifier(base_estimator=sklearn.linear_model.LogisticRegression(), n_estimators = 20)\n",
    "\n",
    "AdaBoost.fit(cancer_X_train_std, cancer_y_train)\n",
    "\n",
    "pred2 = AdaBoost.predict(cancer_X_test_std)\n",
    "\n",
    "print(\"Accuracy for AdaBoostClassifier:\", sklearn.metrics.accuracy_score(cancer_y_test, pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be025c",
   "metadata": {},
   "source": [
    "AdaBoost classifier begins by fitting on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "Here, AdaBoost did not improve accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
